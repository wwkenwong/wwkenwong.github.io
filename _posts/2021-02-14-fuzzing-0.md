---
layout: post
title:  "My first take on real world vulnerability research Part 0"
date:   2021-02-14 12:00:00 +0800
categories: Fuzzing 
---

# Preface 

I am Ken Wong, currently an MPhil student and a leisure CTF player of [@Black Bauhinia](https://twitter.com/BlackB6a), [@VXRL](https://twitter.com/vxresearch), and HKUSTFirebird. 

Early in 2019, for submitting some kinds of applications I started to look for ways to prove my skills. One day, I saw this picture on a slide from [Yuki Chen](https://twitter.com/guhe120) (Below is the same slide I found from twitter).

![]({{ site.url }}/assets/fuzz-0/yukichen.jpg)

"Oh, ‘CV’ is a part of CVE, so CVEs can make my CV complete! "

So I started my journey to look for CVEs. Luckily, we were assigned 2 CVEs after reporting 5 issues to the Apple WebKit: CVE-2019-8678 and CVE-2019-8685. In this blog post series (this is part 1 of 3), I will try to walk through basic ideas and steps on setting up fuzzing, debugging, and exploit development for beginners. We also shared our experience in [HITCON 2020](https://hitcon.org/2020/agenda/1f069ce7-8dad-4aa9-a9f3-20a321a36e34/). 

In this part, I will talk about fuzzing and enumerating targets. 

Disclaimer: The content of this blogpost is written in late 2019, most of them were outdated, it only serves as a reference for beginners. Advised not to read if you have sufficient knowledge on fuzzing. 


# What the fuzz?
Since my goal was to obtain a CVE in time for applications, I decided to go with fuzzing. Fuzzing is an automatic software testing strategy that is used to look for security vulnerabilities in different software by inputting different random data to the targeted software like browser, image parser, OS kernel, etc, and monitor how the input is executed.
In the general case, the vulnerability can only be accounted for a CVE 30-60 days after patching, and fuzzing is the least time-consuming method to discover vulnerabilities. I will recommend this [article](https://labs.f-secure.com/blog/what-the-fuzz/) to know the basics terminology for fuzzing. Anyway, I will list out some of the important terms here.

### Instrumentation : 
The code snippet that inserted into the program to gather execution feedback. Usually, it's used for collecting code coverage, and detect memory corruption. There are mainly two types of instrumentation methods, namely static and dynamic instrumentation. For static instrumentation, it is mainly performed at compile-time, which generally imposes less runtime overhead compare with dynamic instrumentation. The advantage of using dynamic instrumentation is we can directly apply to any binary during runtime DynamoRIO and Pin are two good examples that belong to this category.

### Corpus/Seed : 
Seed/corpus means the initial input files to the fuzzer to carry out mutation. The reason for using seed instead of random data is that random data may not able to pass the sanity check for a lot of parser or fuzzing target. Hence, start with some seed can make everything easier.

### Mutation : 
Basically, given an input, perform some action and generate another input. There is some basic mutation strategy, for example, bit flipping, using dictionaries (for [AFL](https://lcamtuf.blogspot.com/2014/08/binary-fuzzing-strategies-what-works.html)). Some advanced ways, like a tree-based mutation in [Superion](https://arxiv.org/pdf/1812.01197.pdf) or [EMI](https://dl.acm.org/doi/pdf/10.1145/3022671.2984038) used in compiler testing.

### Code Coverage : 
Code coverage is a measurement of how many lines/blocks/arcs of code being executed during fuzzing. There is much different coverage, like instruction coverage, branch coverage, line coverage, etc. In the C/C++ project, we can use gcov to collect coverage by compiling our program with special flags, and we can use lcov to visualize the coverage collected.


# First attempt (Failure)
In the public domain, there are some famous fuzzer, like AFL, Honggfuzz, peach, libfuzzer, etc. With so much choice, libfuzzer and AFL have caught my attention. However, I picked AFL (American Fuzzy Lop) as my first attempt. It is a well-known fuzzing tool and contains many features that I need, including instrumentation, mutation support, code coverage measurement, and forkserver. Most importantly, we don't need to spend time to audit the code and write harness for fuzzing, just like the case for libfuzzer. Still, I will highly recommend this [talk](https://www.youtube.com/watch?v=xzG0pLM4Q64) from [@nedwill](https://twitter.com/NedWilliamson) and his fuzzer for fuzzing chrome IPC, if want to have an introduction to libfuzzer.

Come back to AFL, it can be downloaded [here](http://lcamtuf.coredump.cx/afl/releases/). You can also refer to the [documentation](http://lcamtuf.coredump.cx/afl/QuickStartGuide.txt) for deployment.

P.S. I will recommend starting with [afl++](https://github.com/AFLplusplus/AFLplusplus), since it is actively maintained by the community nowadays......

## TL;DR 

```bash 
sudo apt-get install clang autoconf make cmake
#Build AFL…….
# cd to where you extracted AFL
make 
```

![]({{ site.url }}/assets/fuzz-0/afl_0.jpg)

After built the AFL, add it to the environment by adding the below lines to ~/.profile 

```bash
export PATH=$PATH:<where you build your AFL>
```
To build your fuzzing target,
The first step, config the cc compiler to the AFL's version : 
```bash 
export CC=afl-gcc 
export CXX=afl-g++ 
```
Afterward, continue the remaining software-building steps base on the documentation 

P.S. In AFL, there is a llvm_mode. You can find the code and instructions for building it inside the `llvm_mode` folder of AFL source code. Its report has a 10% gain, though I didn't use it this time. 

![]({{ site.url }}/assets/fuzz-0/afl_1.jpg)

Different software may have slightly different settings, please <b>RTFM</b> a bit before everything is ready. Here are some examples I came across as reference : 

Libjpeg-turbo : 
`cmake -G"Unix Makefiles" -DCMAKE_C_COMPILER=afl-gcc -DCMAKE_C_FLAGS=-m32 `

ImageMagick :
`cmake -DCMAKE_CXX_COMPILER=afl-clang++ -DCMAKE_CC_COMPILER=afl-clang`

Vim (credit to [here](https://groups.google.com/forum/#!topic/vim_dev/YqIhn_PKuXE)) : 

```
CC=afl-gcc ./configure \ --with-features=huge \ --enable-gui=none
make
``` 
Assume you build the setup successfully 

From looking around in the [post](https://lcamtuf.blogspot.com/2014/11/pulling-jpegs-out-of-thin-air.html) of AFL developer   and this [post](https://research.checkpoint.com/2018/50-adobe-cves-in-50-days/) from Checkpoint, I chose to fuzz the JPEG file format as my initial target and targeted on <b>libjpeg-turbo</b>. 
Next, I need to gather some initial corpus. Apart from common sites like 
1. [https://github.com/MozillaSecurity/fuzzdata](https://github.com/MozillaSecurity/fuzzdata)
2. [https://github.com/google/fuzzer-test-suite](https://github.com/google/fuzzer-test-suite) 
3. [https://github.com/libjpeg-turbo/libjpeg-turbo/tree/master/testimages](https://github.com/libjpeg-turbo/libjpeg-turbo/tree/master/testimages) 
4. [https://github.com/fuzzdb-project/fuzzdb/tree/master/attack/file-upload/malicious-images](https://github.com/fuzzdb-project/fuzzdb/tree/master/attack/file-upload/malicious-images) 

I also collected crash samples/regression tests from various image processing libraries and Exploit-DB. With good keywords, you can get some nice samples, for example : 

![]({{ site.url }}/assets/fuzz-0/cve_0.jpg)

![]({{ site.url }}/assets/fuzz-0/cve_1.jpg)

![]({{ site.url }}/assets/fuzz-0/cve_2.jpg)

![]({{ site.url }}/assets/fuzz-0/cve_3.jpg)

At the same time, you can get a sense of which part of your target has a lot of bugs, whether the developers will reward CVE to bugs reported, how long the bugs from reported to being patched. 

![]({{ site.url }}/assets/fuzz-0/cve_4.jpg)

Right now, we had built our target and collected some initial input for the fuzzer. Next, we run this command with the root account (sudo -s) :

`echo core >/proc/sys/kernel/core_pattern`

And, run `afl-cmin` to minimize the corpus. Before fuzz libjpeg-turbo, I fuzzed ImageMagick for a few days to increase the amount of initial corpus. Below is the command to run `afl-cmin` for cjpeg of ImageMagick : 

`afl-cmin -i input/ -o output/ -t 300000 -m 200 ./cjpeg -outfile /dev/null @@`

The @@ in the command is the input field of the original program, and afl will take in @@ as the argument during fuzzing. 
-t is for setting the timeout, -m is for setting the memory limit

After running this command, the minimized output will be saved in ./output/ 

![]({{ site.url }}/assets/fuzz-0/afl_2.jpg)

#### Why need to run cmin? 
With `afl-cmin`, it will give a subset of files but give the same edge coverage. AFL also has a test case minimization function, `afl-tmin`. It will attempt to minimize the size of the input corpus. 


Next, we install `screen`, and we can attach it to the fuzzing processes later (assume using ubuntu). 

`sudo apt install screen `

In my case, I chose to fuzz with 4 cores of my machine and ran the following:

```bash 
# initialize the master
screen afl-fuzz -i input/ -o output/ -M master -t 300000 -m 200 ./cjpeg -outfile /dev/null @@

# initialize other slave 
screen afl-fuzz -i input/ -o output/ -S slave1 -t 300000 -m 200 ./cjpeg -outfile /dev/null @@
screen afl-fuzz -i input/ -o output/ -S slave2 -t 300000 -m 200 ./cjpeg -outfile /dev/null @@
screen afl-fuzz -i input/ -o output/ -S slave3 -t 300000 -m 200 ./cjpeg -outfile /dev/null @@
```
`afl-whatsup` is a handy extension for monitoring all fuzzer's status, and we can use below command :

```bash
afl-whatsup output/ # output is the folder you set in the -o flag when you start running your fuzzer
```

![]({{ site.url }}/assets/fuzz-0/afl_3.jpg)

```bash 
screen -r # for listing process id 
screen -r <id> # for attachment
```

Eg `screen -r 71383` 
Ctrl A+D to leave the process without killing it 

![]({{ site.url }}/assets/fuzz-0/afl_4.jpg)

After 1 week of fuzzing, no crashes were generated. I recalled this blog post from [payatu](https://payatu.com/blog/Nikhil-Joshi/cloudfuzz-machine-learning-powered-content-specific-input-generation-fuzzing), and decided to further mutate the generated corpus in the previous fuzzing. I remembered that [radamsa](https://gitlab.com/akihe/radamsa) can digest files and output mutated files based on the input. 

I tried to re-import the output from fuzzing. Despite gaining coverage, I couldn’t find any crashes. As everyone and academia were using(and modifying) AFL to these famous open source libraries, it would be impossible to generate meaningful crashes without my own AFL modifications.
Here are some excellent academic papers I came across during my self-evaluation: 
1. [Angora](https://web.cs.ucdavis.edu/~hchen/paper/chen2018angora.pdf)
2. [QSYM](https://www.usenix.org/system/files/conference/usenixsecurity18/sec18-yun.pdf)

~~My conclusion: Don't try to mess with things that targeted by academia~~

![]({{ site.url }}/assets/fuzz-0/afl_5.jpg)

# Second attempt (Failure) 
Since I am interested in playing browser CTF challenges, why do I not give a shot to fuzz JavaScript Engines? I recalled the Pwn2Own series of [blogpost](http://blog.ret2.io/2018/06/05/pwn2own-2018-exploit-development/) from ret2system. On their first [blogpost](https://blog.ret2.io/2018/06/13/pwn2own-2018-vulnerability-discovery/), they had demonstrated using [dharma](https://github.com/MozillaSecurity/dharma) for obtaining their Pwn2Own bugs. 

Therefore, I started to modify and extend [Domato](https://github.com/googleprojectzero/domato)‘s grammar to generate pure JavaScript. For those of you who don't know Domato, it is a context-free grammar code generator, originally targeted for DOM fuzzing and engineered by [Ivan Fratric](https://twitter.com/ifsecure) from Google Project Zero. It supports simple and robust PL features with RegEx. Later on, it also supports JScript, VBScript fuzzing and different security researcher also re-engineering it to support various script fuzzing, like PHP, PDF, etc. You may refer to Ivan Fratric's post [here](https://googleprojectzero.blogspot.com/2017/09/the-great-dom-fuzz-off-of-2017.html) for details. 

An advantage of using Domato is the grammar is easy to understand and rewrite. Below is some example copied from its Github repo. And it also supports running python in the grammar. For details on how Domato works, may refer to this excellent analysis [blogpost](https://sigpwn.io/blog/2018/4/14/domato-fuzzers-generation-engine-internals) for Domato's internal.

```
!varformat fuzzvar%05d
!lineguard try { <line> } catch(e) {}

!begin lines
<new element> = document.getElementById("<string min=97 max=122>");
<element>.doSomething();
!end lines

```

Here, I have taken an approach similar to [differential testing](https://en.wikipedia.org/wiki/Differential_testing), namely, we try to execute the same corpus with 2 different builds of v8 and look for the difference in behavior. Of course, no good crashes were generated (despite tons of v8 OOM “features” and panics ). However, by this approach, I found a hang in the v8 engine that only appears in the later build of the v8 engine. Even it's a useless bug, at least we got something this time. 

Even I failed to get some bugs with Domato, we can see [someone](https://blog.redteam.pl/2019/12/chrome-portal-element-fuzzing.html) can do it, I am too weak... 

![]({{ site.url }}/assets/fuzz-0/miner.jpg)

# Third attempt (Somehow successful) 
I was desperate until I saw Saelo’s open-source tool [Fuzzilli](https://github.com/googleprojectzero/fuzzilli)

![]({{ site.url }}/assets/fuzz-0/twitter.jpg)

Fuzzilli is a JS fuzzer targeted at JIT engine bugs. JIT engine is an important part of the Javascript engine, which improves the performance of Javascript execution for frequently executed functions.

![]({{ site.url }}/assets/fuzz-0/jit-compiler.jpg)

In a lot of cases, traditional fuzzer, like Domato or dharma, will wrap the statement inside a try{}catch{}, to prevent exceptions. However, this may prevent the JIT optimization from reducing certain bound checks. Below slides taken from Saelo OffensiveCon presentation cleanly illustrated this idea.

![]({{ site.url }}/assets/fuzz-0/fuzzilli_0.jpg)
![]({{ site.url }}/assets/fuzz-0/fuzzilli_1.jpg)

To solve this issue, Saelo proposed an intermediate language-FuzzIL for the generation of JS code. He implemented the code mutation based on this IL. After that, we can pass these IL to different lifting and generate different corpus, as shown in the below slides: 

![]({{ site.url }}/assets/fuzz-0/fuzzilli_2.jpg)
![]({{ site.url }}/assets/fuzz-0/fuzzilli_3.jpg)

In the initial release, Fuzzilli comes with several mutators, which can be found under `fuzzilli/Sources/Fuzzilli/Mutators/` of fuzzilli. 

![]({{ site.url }}/assets/fuzz-0/fuzzilli_4.jpg)

Now, let’s come back to our goal, this newly released fuzzer reminded me of this old P0 [blogpost](https://googleprojectzero.blogspot.com/2018/10/365-days-later-finding-and-exploiting.html) instantly. Bugs still appear after a year of release of Domato, which means we still have some chance for finding bugs with this good piece of open source work. We understand everyone will look for bugs, we need to narrow down our target to 1 out of the 3 javascript engine and I choose WebKit. 

![]({{ site.url }}/assets/fuzz-0/chakracore.jpg)

My setup steps for fuzzilli (for the initial release): 

```bash 
# first setup the LLVM 
wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | sudo apt-key add -
sudo apt-add-repository "deb http://apt.llvm.org/xenial/ llvm-toolchain-xenial-6.0 main"
sudo apt-get update
sudo apt-get install -y clang-6.0


# next setup swift for building fuzzilli (for old version of swift)
# https://tecadmin.net/install-swift-ubuntu-1604-xenial/
wget https://swift.org/builds/swift-4.0.3-release/ubuntu1604/swift-4.0.3-RELEASE/swift-4.0.3-RELEASE-ubuntu16.04.tar.gz
sudo tar xzf swift-4.0.3-RELEASE-ubuntu16.04.tar.gz
mv swift-4.0.3-RELEASE-ubuntu16.04 /usr/share/swift

echo "export PATH=/usr/share/swift/usr/bin:$PATH" >> ~/.bashrc
source  ~/.bashrc


# next symlink clang 
#https://stackoverflow.com/questions/1951742/how-to-symlink-a-file-in-linux

sudo ln -s /usr/bin/clang-5.0 /usr/bin/clang
sudo ln -s /usr/bin/clang++-5.0 /usr/bin/clang++

# next, install all the dependencies
# you may need to comment some of the dependencies on WebKit's bash
sudo webkit/Tools/gtk/install-dependencies

# next patch the code, copy fuzzbuild.sh to the webkit directory and build the jsc 
patch < webkit.patch
./fuzzbuild.sh

# next cd to fuzzilli's directory 
swift build -Xlinker='-lrt'

# enable this 
echo core >/proc/sys/kernel/core_pattern

# example command to run fuzzilli with 1 master+3 slave
screen ./FuzzilliCli --profile=jsc --storagePath=/home/fuzz/fuzzing1_master8888 --exportCorpus=true --networkMaster=127.0.0.1:8888 /home/fuzz/webkit-master/FuzzBuild/Debug/bin/jsc
screen ./FuzzilliCli --profile=jsc --storagePath=/home/fuzz/fuzzing1_slave8888-1 --exportCorpus=true --networkWorker=127.0.0.1:8888 /home/fuzz/webkit-master/FuzzBuild/Debug/bin/jsc
screen ./FuzzilliCli --profile=jsc --storagePath=/home/fuzz/fuzzing1_slave8888-2 --exportCorpus=true --networkWorker=127.0.0.1:8888 /home/fuzz/webkit-master/FuzzBuild/Debug/bin/jsc
screen ./FuzzilliCli --profile=jsc --storagePath=/home/fuzz/fuzzing1_slave8888-3 --exportCorpus=true --networkWorker=127.0.0.1:8888 /home/fuzz/webkit-master/FuzzBuild/Debug/bin/jsc

```

After fuzzing for a few days, despite the fact that no crashes were generated, I found that the code coverage of initial release Fuzzilli converged towards 27% with very high sample validity (~70%). Below is a plot I prepared for the Hitcon presentation, it illustrated my observation on validity issue(orange line). 

![]({{ site.url }}/assets/fuzz-0/validity.jpg)

Since it's meaningless to run fuzzer with high validity without crashing, I started to modify its mutation strategy and parameters to reduce the correctness of the fuzzer. It turns out this small trick works (although similar patterns also exhibit on later versions of Fuzzilli seem to reduce the validity as well), coverage slightly increases with decreasing validity, and crashes finally happened. 

Part of my patch (with modified parameters and code): 

![]({{ site.url }}/assets/fuzz-0/patch_0.jpg)
![]({{ site.url }}/assets/fuzz-0/patch_1.jpg)
![]({{ site.url }}/assets/fuzz-0/patch_2.jpg)

To scale up the fuzzing, I rented a 12-core machine from GCP and fuzzed for 1.5 months, with 4 cores running the original release as a control setup, and 8 running the modified version (with 1 master + 3 slaves in each setup). It appears that all the security crashes were from the modified version. I am not certain about the reason, it can be luck, possibly P0 or APPLE has fuzzed these targets thoroughly with default config before open-source it to the public. And, since our setup keeps on report crashes after these minor modifications, we spent most of our time on crash triaging and didn't further engineer fuzzilli.

As a reference, our JSC fuzzing campaign lasted for ~2 months in 2019, spent around $1000 GCP credit, which also similar to the value reported by [@ifsecure](https://twitter.com/ifsecure) in his [blogpost](https://googleprojectzero.blogspot.com/2018/10/365-days-later-finding-and-exploiting.html), but with only 2 CVE+ 1 dup in return. 

To gain the whole picture of how the fuzzer works, I suggest going through Saelo’s [master thesis](https://saelo.github.io/papers/thesis.pdf). I learned a lot from this thesis, thanks Saelo :).

In my next blog post (I hope to finish soon :( ), I will describe my analysis on CVE-2019-8678, which I guess this might be one of the variant [case](http://rce.party/wtf.js) posted by qwertyoruiopz. Thanks for reading until here.


~~BTW I don't think the second blogpost will be very soon, just like how my teammate [@mystiz](https://twitter.com/mystiz613) said ...~~

![]({{ site.url }}/assets/fuzz-0/delay.jpg)

Reference

[1] [https://blog.f-secure.com/super-awesome-fuzzing-part-one/](https://blog.f-secure.com/super-awesome-fuzzing-part-one/)

[2] [https://arxiv.org/pdf/1812.00140.pdf](https://arxiv.org/pdf/1812.00140.pdf) 

