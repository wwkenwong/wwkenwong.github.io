---
layout: post
title:  "My first take on real world vulnerability research (Part 1)"
date:   2021-02-14 12:00:00 +0800
categories: Fuzzing 
---

## Preface

I was looking for new challenges that could excite me (and fill my CV) in early 2019. One day, I saw this picture on a slide from [Yuki Chen](https://twitter.com/guhe120).

![](/assets/fuzz-0/yukichen.jpg)

"Oh, CV is inside CVE, so CVEs can make my CV complete!"

Since my goal was to obtain a CVE within a short period of time, I decided to go with fuzzing. That is how I start my journey to look for CVEs. Luckily, we were assigned 2 CVEs after reporting 5 issues to Apple WebKit, namely CVE-2019-8678 and CVE-2019-8685. In this series of blog posts, I will walk through some basic ideas of fuzzing. We will also get started on fuzzing, debugging and developing exploits. We have also shared our experience in [HITCON 2020](https://hitcon.org/2020/agenda/1f069ce7-8dad-4aa9-a9f3-20a321a36e34/). In the first part, we will look into fuzzing and enumerating targets.

_Disclaimer: The content of this blogpost is written in late 2019 and some of them are not up-to-date. It only serves as a reference for beginners. You will not need this if you have sufficient knowledge on fuzzing, anyway._

## What the fuzz?

Fuzzing is an automatic testing strategy used to look for security vulnerabilities. In short, this is a methodology to analyse software behaviour by sending it random data.

In general, a vulnerability can only be accounted for CVE 30-60 days after patching, and fuzzing is the least time-consuming method to discover vulnerabilities. I recommend this [article](https://labs.f-secure.com/blog/what-the-fuzz/) for some terminology for fuzzing. I will just cover four of the important terms here.

**Instrumentation** is a code snippet that is inserted into the program to gather execution feedback. Usually, it is used for collecting code coverage and detect memory corruption. There are two types of instrumentation methods, static and dynamic instrumentation. For static instrumentation, it is performed at compile-time, which imposes less runtime overhead compared to dynamic instrumentation. On the other hand, one of the benefits of dynamic instrumentation is that they are easily applied to binaries during runtime. _DynamoRIO_ and _Pin_ are two examples of instrumentation tools.

**Seed (or corpus)** is the initial input files to the fuzzer to carry out mutation. The reason for using seed instead of random data is that completely random input may not be able to pass the sanity check for a lot of parser or fuzzing target. Seeds are used to make the results of fuzzing more meaningful.

**Mutation** transforms an input into new input. There are some well-known and simple mutation strategies, for example, bit flipping and embed tokens from user-defined dictionaries. Both of them were implemented in [AFL](https://lcamtuf.blogspot.com/2014/08/binary-fuzzing-strategies-what-works.html). There are also some advanced strategies like tree-based mutations used in [Superion](https://arxiv.org/pdf/1812.01197.pdf) or [EMI](https://dl.acm.org/doi/pdf/10.1145/3022671.2984038).

**Code coverage** is a measurement of the proportion of the code being executed. There are various level of coverages like instruction coverage, branch coverage, line coverage. In C/C++ projects, we can use _gcov_ to collect coverage by compiling our program with special flags, and we can use _lcov_ to visualize the coverage collected.

## First attempt (Failure)

There are some state-of-the-art fuzzers in the public domain like _AFL_, _Honggfuzz_, _peach_, _libfuzzer_. With so much choice, _libfuzzer_ and _AFL_ have caught my attention. However, I picked _AFL_ (American Fuzzy Lop) as my first attempt. It is a well-known fuzzing tool and contains many features that I need. This includes instrumentation, mutation, code coverage measurement and forkserver. Most importantly, we don't need to spend time to audit the code and write harness for fuzzing, just like the case for _libfuzzer_. If you want to have an introduction to libfuzzer, I highly recommend [this talk](https://www.youtube.com/watch?v=xzG0pLM4Q64) from [@nedwill](https://twitter.com/NedWilliamson) and his fuzzer for Chrome IPC. Anyway, let's get back to _AFL_, it is available [here](http://lcamtuf.coredump.cx/afl/releases/). You can also refer to [the documentation](http://lcamtuf.coredump.cx/afl/QuickStartGuide.txt) for deployment.

_Note: I will recommend starting with [afl++](https://github.com/AFLplusplus/AFLplusplus) since it is actively maintained by the community nowadays..._

### Setting up

```bash 
sudo apt-get install clang autoconf make cmake
# Build AFL...
# cd to where you extracted AFL
make 
```

![](/assets/fuzz-0/afl_0.jpg)

After AFL is built, add the below line to `~/.profile`. 

```bash
export PATH=$PATH:<where you build your AFL>
```

To build your fuzzing target, configure the _cc compiler_ to the AFL's version:

```bash 
export CC=afl-gcc 
export CXX=afl-g++ 
```

Continue the remaining steps from the documentation to build the application.

_Note: In AFL, there is a LLVM mode. You can find the code and instructions for building it inside the `llvm_mode` folder of AFL source code. Its report has a 10% increase in performance when compare with default mode, though I didn't use it this time._

![](/assets/fuzz-0/afl_1.jpg)

Different software may have slightly different settings, just _RTFM_ before everything is ready. Here are some examples I came across as reference: 

```bash
# For libjpeg-turbo
cmake -G"Unix Makefiles" -DCMAKE_C_COMPILER=afl-gcc -DCMAKE_C_FLAGS=-m32

# For ImageMagick
cmake -DCMAKE_CXX_COMPILER=afl-clang++ -DCMAKE_CC_COMPILER=afl-clang

# From Vim
CC=afl-gcc ./configure \ --with-features=huge \ --enable-gui=none
make
```

From looking around in [the post](https://lcamtuf.blogspot.com/2014/11/pulling-jpegs-out-of-thin-air.html) of _AFL_ developer and [this post](https://research.checkpoint.com/2018/50-adobe-cves-in-50-days/) from _Checkpoint_, I chose to fuzz the JPEG file format as my initial target and targeted on _libjpeg-turbo_. I need to gather some initial corpus from the below GitHub repositories:

1. [https://github.com/MozillaSecurity/fuzzdata](https://github.com/MozillaSecurity/fuzzdata)
2. [https://github.com/google/fuzzer-test-suite](https://github.com/google/fuzzer-test-suite) 
3. [https://github.com/libjpeg-turbo/libjpeg-turbo/tree/master/testimages](https://github.com/libjpeg-turbo/libjpeg-turbo/tree/master/testimages) 
4. [https://github.com/fuzzdb-project/fuzzdb/tree/master/attack/file-upload/malicious-images](https://github.com/fuzzdb-project/fuzzdb/tree/master/attack/file-upload/malicious-images) 

I also collected crash samples and regression tests from various image processing libraries and Exploit-DB. We are able to get some nice samples with correct keywords. For instance:

![](/assets/fuzz-0/cve_0.jpg)

![](/assets/fuzz-0/cve_1.jpg)

![](/assets/fuzz-0/cve_2.jpg)

![](/assets/fuzz-0/cve_3.jpg)

At the same time, you can get a sense of the following questions:
* which part of your target had lots of bugs,
* whether the developers will reward CVE to bugs reported, and
* how long does it take for the developers to fix the bugs.

![](/assets/fuzz-0/cve_4.jpg)

For now, we had built our target and collected some initial input for the fuzzer. Next, we run this command with the root account (`sudo -s`):

```
echo core >/proc/sys/kernel/core_pattern
```

After that, run `afl-cmin` to minimize the corpus. Before fuzzing `libjpeg-turbo`, I fuzzed `ImageMagick` for a few days to collect more initial corpus. Below is the command to run `afl-cmin` for cjpeg of ImageMagick: 

```
afl-cmin -i input/ -o output/ -t 300000 -m 200 ./cjpeg -outfile /dev/null @@
```

The `@@` in the command is the input field of the original program, and afl will take in `@@` as the argument during fuzzing. Also, `-t` is for setting the timeout and `-m` is for setting the memory limit.

After running this command, the minimized output will be saved in `./output/`.

![](/assets/fuzz-0/afl_2.jpg)

#### Why cmin? 

`afl-cmin` generates a subset of files that yield the same edge coverage. _AFL_ also has a test case minimization function, `afl-tmin`. It will attempt to minimize the size of the corpus. Next, we install `screen`, and we can attach it to the fuzzing processes later (assume using ubuntu). 

```
sudo apt install screen
```

In my case, I chose to fuzz with 4 cores of my machine and ran the following:

```bash 
# initialize the master
screen afl-fuzz -i input/ -o output/ -M master -t 300000 -m 200 ./cjpeg -outfile /dev/null @@

# initialize other slave 
screen afl-fuzz -i input/ -o output/ -S slave1 -t 300000 -m 200 ./cjpeg -outfile /dev/null @@
screen afl-fuzz -i input/ -o output/ -S slave2 -t 300000 -m 200 ./cjpeg -outfile /dev/null @@
screen afl-fuzz -i input/ -o output/ -S slave3 -t 300000 -m 200 ./cjpeg -outfile /dev/null @@
```

`afl-whatsup` is a handy extension for monitoring the status of the fuzzers. We can use the command below:

```bash
afl-whatsup output/ # output is the folder you set in the -o flag when you start running your fuzzer
```

![](/assets/fuzz-0/afl_3.jpg)

```bash 
screen -r # for listing process id 
screen -r <id> # for attachment
```

![](/assets/fuzz-0/afl_4.jpg)

Sadly, no crashes were generated after a week of fuzzing. I recalled this blog post from [payatu](https://payatu.com/blog/Nikhil-Joshi/cloudfuzz-machine-learning-powered-content-specific-input-generation-fuzzing) and decided to further mutate the generated corpus. I remembered that [radamsa](https://gitlab.com/akihe/radamsa) can digest files and output mutated files based on the input. Therefore, I used the generated corpus from previous fuzzing to `radamsa` and generate more corpus.

I tried to re-import the output from `radamsa`. Despite gaining coverage, I couldnâ€™t find any crashes. As everyone and academia were using (and modifying) _AFL_ on these famous open source libraries, it would be impossible to generate meaningful crashes without my own AFL modifications. [Angora](https://web.cs.ucdavis.edu/~hchen/paper/chen2018angora.pdf) and [QSYM](https://www.usenix.org/system/files/conference/usenixsecurity18/sec18-yun.pdf) are two excellent academic papers I came across during my self-evaluation. ~~Well, don't try to mess with things that targeted by academia.~~

![](/assets/fuzz-0/afl_5.jpg)

## Second attempt (Failure) 

Since I am interested in playing browser CTF challenges, why don't I give a shot to fuzz JavaScript Engines? I recalled the [blogpost series on _Pwn2Own_](http://blog.ret2.io/2018/06/05/pwn2own-2018-exploit-development/) from _ret2system_. They had demonstrated using [dharma](https://github.com/MozillaSecurity/dharma) on their [first blogpost](https://blog.ret2.io/2018/06/13/pwn2own-2018-vulnerability-discovery/) for the bugs they submitted to _Pwn2Own_.

Therefore, I started to modify and extend [Domato](https://github.com/googleprojectzero/domato)'s grammar to generate pure JavaScript. For those of you who don't know Domato, it is a context-free grammar code generator originally targeted for DOM fuzzing, developed by [Ivan Fratric](https://twitter.com/ifsecure) from Google Project Zero. It supports simple and robust PL features with regular expressions. It also supports JScript, VBScript fuzzing in a later stage. Some security researchers re-engineering it to support more templates like PHP and PDF. You may refer to [Ivan Fratric's post](https://googleprojectzero.blogspot.com/2017/09/the-great-dom-fuzz-off-of-2017.html) for details. 

An advantage of using Domato is the grammar is easy to understand and rewrite. Below is some example copied from its Github repo. And it also supports running Python in the grammar. For details on how Domato works, you may refer to this excellent analysis [blog post](https://sigpwn.io/blog/2018/4/14/domato-fuzzers-generation-engine-internals) for Domato's internal.

```
!varformat fuzzvar%05d
!lineguard try { <line> } catch(e) {}

!begin lines
<new element> = document.getElementById("<string min=97 max=122>");
<element>.doSomething();
!end lines
```

Here, I have taken an approach similar to [differential testing](https://en.wikipedia.org/wiki/Differential_testing). We try to execute the same corpus with two different builds of v8 and look for the difference in behaviour. No good crashes were generated despite tons of v8 OOM features and panics. However, I found a hang in the v8 engine that only appears in the later builds using this approach. Even though it is useless, at least we got something this time.

Although I failed to get some bugs with Domato, there are [some successful attempts](https://blog.redteam.pl/2019/12/chrome-portal-element-fuzzing.html). I am just too weak...

![](/assets/fuzz-0/miner.jpg)

## Third attempt (Succeed, maybe?)

I was desperate until I saw Saelo's open-source tool [Fuzzilli](https://github.com/googleprojectzero/fuzzilli).

![](/assets/fuzz-0/twitter.jpg)

Fuzzilli is a JS fuzzer targeted at JIT engine bugs. JIT engine is an important part of the Javascript engine, which improves the performance of Javascript execution for frequently executed functions.

![](/assets/fuzz-0/jit-compiler.jpg)

Traditional fuzzer, like Domato or dharma, wrap the statements inside a try-catch block to prevent exceptions. However, this may prevent the JIT _Bounds Check Elimination (BCE)_ optimization. The slides below from _Saelo_ for _OffensiveCon_ illustrated this idea.

![](/assets/fuzz-0/fuzzilli_0.jpg)
![](/assets/fuzz-0/fuzzilli_1.jpg)

To resolve this, _Saelo_ proposed an intermediate language - _FuzzIL_ for generating JS code. He implemented the code mutation based on this IL. After that, we can pass these IL to different lifting and generate different corpus, as shown in the below slides:

![](/assets/fuzz-0/fuzzilli_2.jpg)
![](/assets/fuzz-0/fuzzilli_3.jpg)

_Fuzzilli_ comes with several mutators in the initial release, which can be found under `./fuzzilli/Sources/Fuzzilli/Mutators/`.

![](/assets/fuzz-0/fuzzilli_4.jpg)

Now, let's get back to our goal. This newly released fuzzer reminded me of [this post](https://googleprojectzero.blogspot.com/2018/10/365-days-later-finding-and-exploiting.html) of Project Zero instantly. Bugs still appear after a year of release of Domato, which means that we may be able to find some bugs. We must first decide on our target as everyone out there are finding bugs at the same time. I ultimately chose WebKit among popular JS engines to fuzz.

![](/assets/fuzz-0/chakracore.jpg)

My setup steps for fuzzilli (for the initial release):

```bash 
# first setup the LLVM 
wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | sudo apt-key add -
sudo apt-add-repository "deb http://apt.llvm.org/xenial/ llvm-toolchain-xenial-6.0 main"
sudo apt-get update
sudo apt-get install -y clang-6.0

# next setup swift for building fuzzilli (for old version of swift)
# https://tecadmin.net/install-swift-ubuntu-1604-xenial/
wget https://swift.org/builds/swift-4.0.3-release/ubuntu1604/swift-4.0.3-RELEASE/swift-4.0.3-RELEASE-ubuntu16.04.tar.gz
sudo tar xzf swift-4.0.3-RELEASE-ubuntu16.04.tar.gz
mv swift-4.0.3-RELEASE-ubuntu16.04 /usr/share/swift

echo "export PATH=/usr/share/swift/usr/bin:$PATH" >> ~/.bashrc
source ~/.bashrc

# next symlink clang 
#https://stackoverflow.com/questions/1951742/how-to-symlink-a-file-in-linux

sudo ln -s /usr/bin/clang-5.0 /usr/bin/clang
sudo ln -s /usr/bin/clang++-5.0 /usr/bin/clang++

# next, install all the dependencies
# you may need to comment out some of the dependencies on WebKit's bash
sudo webkit/Tools/gtk/install-dependencies

# next patch the code, copy fuzzbuild.sh to the webkit directory and build the jsc 
patch < webkit.patch
./fuzzbuild.sh

# next cd to fuzzilli's directory 
swift build -Xlinker='-lrt'

# enable this 
echo core >/proc/sys/kernel/core_pattern

# example command to run fuzzilli with 1 master+3 slave
screen ./FuzzilliCli --profile=jsc --storagePath=/home/fuzz/fuzzing1_master8888 --exportCorpus=true --networkMaster=127.0.0.1:8888 /home/fuzz/webkit-master/FuzzBuild/Debug/bin/jsc
screen ./FuzzilliCli --profile=jsc --storagePath=/home/fuzz/fuzzing1_slave8888-1 --exportCorpus=true --networkWorker=127.0.0.1:8888 /home/fuzz/webkit-master/FuzzBuild/Debug/bin/jsc
screen ./FuzzilliCli --profile=jsc --storagePath=/home/fuzz/fuzzing1_slave8888-2 --exportCorpus=true --networkWorker=127.0.0.1:8888 /home/fuzz/webkit-master/FuzzBuild/Debug/bin/jsc
screen ./FuzzilliCli --profile=jsc --storagePath=/home/fuzz/fuzzing1_slave8888-3 --exportCorpus=true --networkWorker=127.0.0.1:8888 /home/fuzz/webkit-master/FuzzBuild/Debug/bin/jsc
```

After fuzzing for a few days, although no crashes were generated, I found that the code coverage converged to 27% with very high sample validity. Below is a plot I prepared for the HITCON presentation, the orange line validated my observation on validity issue.

![](/assets/fuzz-0/validity.jpg)

Since it's meaningless to run fuzzers with high validity without crashing, I started to modify its mutation strategy and parameters to reduce the correctness of the fuzzer. It turns out this small trick works and coverage has increased slightly with decreasing validity and crashes finally happened.

Part of my patch (with modified parameters and code):

![](/assets/fuzz-0/patch_0.jpg)
![](/assets/fuzz-0/patch_1.jpg)
![](/assets/fuzz-0/patch_2.jpg)

To scale up the fuzzing, I rented a 12-core machine from _GCP_ and fuzzed for 1.5 months, with 4 cores running the original release as a control setup, and 8 running the modified version (with 1 master + 3 slaves in each setup). It appears that all the security crashes were from the modified version. I am not certain about the reason, it can be luck, possibly P0 or Apple has fuzzed these targets thoroughly with default configuration before open-sourcing it to the public. Since our setup keeps on report crashes after these minor modifications, we spent most of our time on crash triaging them and didn't further modifing fuzzilli.

As a reference, our JSC fuzzing campaign lasted for around 2 months in 2019, we spent around $1000 GCP credit, which is also similar to the cost reported by [@ifsecure](https://twitter.com/ifsecure) in [his blogpost](https://googleprojectzero.blogspot.com/2018/10/365-days-later-finding-and-exploiting.html). In return, we harvested 2 CVEs.

To gain the whole picture of how the fuzzer works, I suggest going through [Saelo's master thesis](https://saelo.github.io/papers/thesis.pdf). Thanks Saelo and I learned a lot from his thesis.

## Epilogue

In my next blog post (which I hope to finish as soon as possible), I will analyse on CVE-2019-8678, which I guess this might be one of the variants of [wtf.js](http://rce.party/wtf.js) posted by _qwertyoruiopz_. Thanks for reading until here.

_By the way, as my teammate [@mystiz](https://twitter.com/mystiz613) said, part two of the blog post may not appear any time soon. This episode is already delayed by an year already._

![](/assets/fuzz-0/delay.jpg)

## Reference

1. https://blog.f-secure.com/super-awesome-fuzzing-part-one/
2. https://arxiv.org/pdf/1812.00140.pdf
